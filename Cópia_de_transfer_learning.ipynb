{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92mQ2gMlYoZl"
      },
      "source": [
        "# Transfer learning / fine-tuning\n",
        "\n",
        "Este tutorial o guiará pelo processo de uso do aprendizado de transferência para aprender um classificador de imagem preciso a partir de um número relativamente pequeno de amostras de treinamento. De um modo geral, a aprendizagem de transferência refere-se ao processo de alavancar o conhecimento aprendido em um modelo para o treinamento de outro modelo.\n",
        "Mais especificamente, o processo envolve pegar uma rede neural existente que foi previamente treinada para um bom desempenho em um conjunto de dados maior e usá-la como base para um novo modelo que aproveita a precisão da rede anterior para uma nova tarefa. Este método tornou-se popular nos últimos anos para melhorar o desempenho de uma rede neural treinada em um pequeno conjunto de dados; a intuição é que o novo conjunto de dados pode ser muito pequeno para ser treinado para um bom desempenho por si só, mas sabemos que a maioria das redes neurais treinadas para aprender recursos de imagem geralmente aprendem recursos semelhantes de qualquer maneira, especialmente nas camadas iniciais, onde são mais genéricos (detectores de borda, bolhas, e assim por diante).\n",
        "\n",
        "\n",
        "A aprendizagem por transferência foi amplamente possibilitada pelo código aberto de modelos de última geração; para os modelos de melhor desempenho em tarefas de classificação de imagens  (como do [ILSVRC](http://www.image-net.org/challenges/LSVRC/)), é prática comum agora não apenas publicar a arquitetura, mas também liberar os pesos treinados do modelo. Isso permite que amadores usem esses classificadores de imagem para aumentar o desempenho de seus próprios modelos específicos de tarefas.\n",
        "\n",
        "#### Feature extraction vs. fine-tuning\n",
        "\n",
        "Em um extremo, o aprendizado de transferência pode envolver pegar a rede pré-treinada e congelar os pesos e usar uma de suas camadas ocultas (geralmente a última) como um extrator de recursos, usando esses recursos como entrada para uma rede neural menor.\n",
        "No outro extremo, começamos com a rede pré-treinada, mas permitimos que alguns dos pesos (geralmente a última camada ou as últimas camadas) sejam modificados. Outro nome para este procedimento é chamado de \"ajuste fino\" porque estamos ajustando levemente os pesos da rede pré-treinada para a nova tarefa. Normalmente treinamos essa rede com uma taxa de aprendizado menor, pois esperamos que os recursos já sejam relativamente bons e não precisem ser muito alterados.\n",
        "\n",
        "\n",
        "Às vezes, fazemos algo intermediário: congelar apenas as camadas iniciais/genéricas, mas ajustar as camadas posteriores. Qual estratégia é a melhor depende do tamanho do seu conjunto de dados, do número de classes e do quanto ele se assemelha ao conjunto de dados no qual o modelo anterior foi treinado (e, portanto, se ele pode se beneficiar dos mesmos extratores de recursos aprendidos). Uma discussão mais detalhada de como criar estratégias pode ser encontrada em [[1]](http://cs231n.github.io/transfer-learning/) [[2]](http://sebastianruder.com/transfer-learning/).\n",
        "\n",
        "## Procedure\n",
        "\n",
        "Neste guia, passaremos pelo processo de carregamento de um classificador de imagens de 1000 classes de última geração, [VGG16](https://arxiv.org/pdf/1409.1556.pdf) que venceu o [ImageNet challenge em 2014](http://www.robots.ox.ac.uk/~vgg/research/very_deep/), e o uso como um extrator de recursos fixos para treinar um classificador personalizado menor em nosso suas próprias imagens, embora com muito poucas alterações de código, você também pode tentar ajustar.\n",
        "\n",
        "Primeiro vamos carregar o VGG16 e remover sua camada final, a camada de classificação softmax de 1000 classes específica para ImageNet, e substituí-la por uma nova camada de classificação para as classes sobre as quais estamos treinando. Em seguida, congelaremos todos os pesos na rede, exceto os novos que se conectam à nova camada de classificação, e treinaremos a nova camada de classificação sobre nosso novo conjunto de dados.\n",
        "\n",
        "Também compararemos esse método com o treinamento de uma pequena rede neural do zero no novo conjunto de dados e, como veremos, melhorará drasticamente nossa precisão. Faremos essa parte primeiro.\n",
        "\n",
        "Como nosso sujeito de teste, usaremos um conjunto de dados composto por cerca de 5960 imagens pertencentes a 97 classes e treinaremos um classificador de imagens com cerca de 80% de precisão. Vale a pena notar que essa estratégia se adapta bem a conjuntos de imagens em que você pode ter apenas algumas centenas ou menos imagens. Seu desempenho será menor a partir de um pequeno número de amostras (dependendo das classes) como de costume, mas ainda impressionante considerando as restrições usuais.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3p-OjhDPYoZm"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "\n",
        "#if using Theano with GPU\n",
        "#os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.models import Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWWN-FPLYoZs"
      },
      "source": [
        "### Getting a dataset\n",
        "\n",
        "O primeiro passo será carregar nossos dados. Como nosso exemplo, usaremos o conjunto de dados [CalTech-101](http://www.vision.caltech.edu/datasets/), que contém cerca de 9.000 imagens rotuladas pertencentes a 101 categorias de objetos. No entanto, excluiremos 6 das categorias que tiverem mais imagens. Isso é para manter a distribuição de classes bastante equilibrada (em torno de 50-100) e restrita a um número menor de imagens, em torno de 5960.\n",
        "\n",
        "Se você deseja usar seu próprio conjunto de dados, ele deve ser organizado da mesma forma `101_ObjectCategories` com todas as imagens organizadas em subpastas, uma para cada classe. Nesse caso, a célula a seguir deve carregar seu conjunto de dados personalizado corretamente apenas substituindo `root` por sua pasta. Se você tiver uma estrutura alternativa, você só precisa ter certeza de carregar a lista `data` onde cada elemento é um dict onde `x` estão os dados (um array numpy 1-d) e `y` é o rótulo (um inteiro). Use a função auxiliar `get_image(path)`para carregar a imagem corretamente no array e observe também que as imagens estão sendo redimensionadas para 224x224. Isso é necessário porque a entrada para VGG16 é uma imagem RGB de 224x224. Você não precisa redimensioná-los em seu disco rígido, pois isso está sendo feito no código abaixo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XklKIrnaZb3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c51e9e14-4b27-4b70-c823-4581173e802a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 101_Object_Categories for image notebooks\n",
            "######################################################################## 100.0%\n",
            "101_ObjectCategories  sample_data\n"
          ]
        }
      ],
      "source": [
        "!echo \"Downloading 101_Object_Categories for image notebooks\"\n",
        "!curl -L -o 101_ObjectCategories.tar.gz --progress-bar https://github.com/JulianoMata/dio-treinamento-de-redes-neurais/blob/main/Dados/101_ObjectCategories.tar.gz?raw=true\n",
        "!tar -xzf 101_ObjectCategories.tar.gz\n",
        "!rm 101_ObjectCategories.tar.gz\n",
        "!ls\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OGRcLNwYoZu",
        "outputId": "4df13231-76d1-43ab-cd0b-737285e43a9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/101_ObjectCategories/menorah', '/content/101_ObjectCategories/gerenuk', '/content/101_ObjectCategories/inline_skate', '/content/101_ObjectCategories/minaret', '/content/101_ObjectCategories/ferry', '/content/101_ObjectCategories/yin_yang', '/content/101_ObjectCategories/mayfly', '/content/101_ObjectCategories/hedgehog', '/content/101_ObjectCategories/schooner', '/content/101_ObjectCategories/crayfish', '/content/101_ObjectCategories/bonsai', '/content/101_ObjectCategories/water_lilly', '/content/101_ObjectCategories/pyramid', '/content/101_ObjectCategories/elephant', '/content/101_ObjectCategories/camera', '/content/101_ObjectCategories/crocodile_head', '/content/101_ObjectCategories/rooster', '/content/101_ObjectCategories/tick', '/content/101_ObjectCategories/anchor', '/content/101_ObjectCategories/flamingo', '/content/101_ObjectCategories/flamingo_head', '/content/101_ObjectCategories/revolver', '/content/101_ObjectCategories/headphone', '/content/101_ObjectCategories/dalmatian', '/content/101_ObjectCategories/emu', '/content/101_ObjectCategories/beaver', '/content/101_ObjectCategories/dollar_bill', '/content/101_ObjectCategories/barrel', '/content/101_ObjectCategories/scorpion', '/content/101_ObjectCategories/trilobite', '/content/101_ObjectCategories/sea_horse', '/content/101_ObjectCategories/platypus', '/content/101_ObjectCategories/snoopy', '/content/101_ObjectCategories/laptop', '/content/101_ObjectCategories/panda', '/content/101_ObjectCategories/metronome', '/content/101_ObjectCategories/cup', '/content/101_ObjectCategories/sunflower', '/content/101_ObjectCategories/pagoda', '/content/101_ObjectCategories/airplanes', '/content/101_ObjectCategories/Leopards', '/content/101_ObjectCategories/dolphin', '/content/101_ObjectCategories/joshua_tree', '/content/101_ObjectCategories/umbrella', '/content/101_ObjectCategories/Motorbikes', '/content/101_ObjectCategories/wild_cat', '/content/101_ObjectCategories/brontosaurus', '/content/101_ObjectCategories/butterfly', '/content/101_ObjectCategories/crocodile', '/content/101_ObjectCategories/strawberry', '/content/101_ObjectCategories/car_side', '/content/101_ObjectCategories/lamp', '/content/101_ObjectCategories/soccer_ball', '/content/101_ObjectCategories/accordion', '/content/101_ObjectCategories/pigeon', '/content/101_ObjectCategories/chair', '/content/101_ObjectCategories/helicopter', '/content/101_ObjectCategories/ceiling_fan', '/content/101_ObjectCategories/octopus', '/content/101_ObjectCategories/llama', '/content/101_ObjectCategories/cougar_body', '/content/101_ObjectCategories/Faces_easy', '/content/101_ObjectCategories/cannon', '/content/101_ObjectCategories/wrench', '/content/101_ObjectCategories/hawksbill', '/content/101_ObjectCategories/binocular', '/content/101_ObjectCategories/grand_piano', '/content/101_ObjectCategories/lotus', '/content/101_ObjectCategories/garfield', '/content/101_ObjectCategories/ewer', '/content/101_ObjectCategories/stop_sign', '/content/101_ObjectCategories/ant', '/content/101_ObjectCategories/watch', '/content/101_ObjectCategories/brain', '/content/101_ObjectCategories/euphonium', '/content/101_ObjectCategories/ketch', '/content/101_ObjectCategories/gramophone', '/content/101_ObjectCategories/buddha', '/content/101_ObjectCategories/kangaroo', '/content/101_ObjectCategories/ibis', '/content/101_ObjectCategories/okapi', '/content/101_ObjectCategories/bass', '/content/101_ObjectCategories/stegosaurus', '/content/101_ObjectCategories/starfish', '/content/101_ObjectCategories/stapler', '/content/101_ObjectCategories/cellphone', '/content/101_ObjectCategories/windsor_chair', '/content/101_ObjectCategories/mandolin', '/content/101_ObjectCategories/wheelchair', '/content/101_ObjectCategories/dragonfly', '/content/101_ObjectCategories/lobster', '/content/101_ObjectCategories/scissors', '/content/101_ObjectCategories/rhino', '/content/101_ObjectCategories/saxophone', '/content/101_ObjectCategories/nautilus', '/content/101_ObjectCategories/pizza', '/content/101_ObjectCategories/electric_guitar']\n"
          ]
        }
      ],
      "source": [
        "dados = ('/content/101_ObjectCategories')\n",
        "train_split, val_split = 0.7, 0.15\n",
        "\n",
        "categories = [x[0] for x in os.walk(dados) if x[0]][1:]\n",
        "categories = [c for c in categories]\n",
        "\n",
        "print(categories)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2ERhVlFYoZy"
      },
      "source": [
        "Esta função é útil para pré-processar os dados em uma imagem e vetor de entrada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "A1T1Joq7YoZz"
      },
      "outputs": [],
      "source": [
        "# helper function to load image and return it and input vector\n",
        "def get_image(path):\n",
        "    img = keras.utils.load_img(path, target_size=(224, 224))\n",
        "    # img = image.load_img(path, target_size=(224, 224)) --> 'erro na execução'\n",
        "    x = keras.utils.img_to_array(img)\n",
        "    # x = image.img_to_array(img) --> 'erro na excecução'\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    x = preprocess_input(x)\n",
        "    return img, x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUwQ60GGYoZ3"
      },
      "source": [
        "Carregue todas as imagens da pasta raiz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5nAUr-ooYoZ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf04bae2-79ad-4f85-bd40-08fc7c661462"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temos \u001b[1;32m97\u001b[m classes\n"
          ]
        }
      ],
      "source": [
        "data = []\n",
        "for c, category in enumerate(categories):\n",
        "    images = [os.path.join(dp, f) for dp, dn, filenames \n",
        "              in os.walk(category) for f in filenames \n",
        "              if os.path.splitext(f)[1].lower() in ['.jpg','.png','.jpeg']]\n",
        "    for img_path in images:\n",
        "        img, x = get_image(img_path)\n",
        "        data.append({'x':np.array(x[0]), 'y':c})\n",
        "\n",
        "# count the number of classes\n",
        "num_classes = len(categories)\n",
        "print(f'Temos \\033[1;32m{num_classes}\\033[m classes')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55Rw-ptVYoZ7"
      },
      "source": [
        "Randomize a ordem dos dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5vGeJK55YoZ8"
      },
      "outputs": [],
      "source": [
        "random.shuffle(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwHqS_NgYoZ_"
      },
      "source": [
        "Criar treinamento / validação / divisão de teste (70%, 15%, 15%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PT9Cuq2rYoaB"
      },
      "outputs": [],
      "source": [
        "idx_val = int(train_split * len(data))\n",
        "idx_test = int((train_split + val_split) * len(data))\n",
        "train = data[:idx_val]\n",
        "val = data[idx_val:idx_test]\n",
        "test = data[idx_test:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsOVhpqcYoaF"
      },
      "source": [
        "Dados separados para rótulos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQOGN9kOYoaH",
        "outputId": "f7a89a57-9d69-4c36-ca3a-0f45686714f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[73, 3, 39, 21, 1, 83, 94, 86, 38, 44, 44, 3, 83, 44, 8, 5, 81, 61, 72, 61, 75, 39, 67, 75, 61, 56, 14, 56, 44, 5, 1, 88, 77, 64, 38, 39, 91, 83, 44, 12, 66, 33, 48, 93, 40, 89, 53, 44, 44, 2, 61, 44, 74, 84, 44, 36, 57, 29, 53, 85, 67, 44, 57, 55, 47, 79, 44, 80, 21, 44, 33, 61, 44, 16, 25, 27, 4, 86, 23, 30, 71, 39, 36, 61, 44, 23, 45, 51, 10, 23, 39, 44, 72, 33, 86, 16, 14, 50, 75, 27, 40, 40, 25, 13, 39, 51, 3, 61, 0, 73, 56, 39, 61, 21, 39, 39, 56, 41, 37, 50, 13, 56, 37, 21, 25, 83, 72, 90, 47, 23, 89, 53, 95, 47, 27, 40, 40, 70, 79, 77, 44, 73, 66, 0, 17, 59, 9, 64, 70, 29, 59, 0, 44, 61, 44, 4, 39, 3, 78, 81, 77, 77, 72, 8, 44, 72, 33, 92, 59, 72, 83, 44, 92, 68, 44, 10, 44, 37, 39, 83, 6, 82, 7, 0, 4, 40, 40, 19, 29, 44, 39, 81, 32, 50, 44, 9, 39, 72, 37, 15, 44, 70, 35, 24, 44, 90, 81, 63, 14, 66, 39, 15, 39, 61, 8, 4, 21, 3, 25, 78, 55, 33, 39, 10, 23, 65, 39, 50, 39, 39, 61, 7, 65, 55, 44, 13, 64, 72, 16, 94, 50, 40, 39, 44, 82, 41, 92, 44, 47, 75, 24, 40, 30, 40, 39, 75, 18, 87, 72, 67, 39, 56, 41, 75, 35, 39, 74, 44, 44, 91, 21, 26, 69, 61, 6, 39, 50, 39, 80, 41, 44, 78, 8, 15, 61, 77, 69, 0, 47, 78, 50, 63, 23, 52, 89, 60, 15, 47, 44, 28, 68, 81, 41, 61, 61, 47, 48, 92, 83, 43, 3, 52, 94, 50, 48, 44, 2, 25, 39, 36, 84, 72, 10, 66, 8, 85, 82, 69, 58, 12, 72, 44, 4, 64, 79, 44, 61, 44, 71, 44, 40, 68, 88, 61, 4, 5, 92, 17, 39, 75, 44, 38, 59, 56, 44, 13, 69, 48, 1, 81, 38, 55, 44, 20, 12, 44, 72, 57, 32, 75, 73, 44, 72, 88, 44, 10, 65, 51, 40, 22, 39, 61, 44, 43, 44, 72, 20, 83, 86, 73, 24, 61, 83, 39, 49, 44, 32, 44, 44, 10, 44, 40, 1, 93, 75, 44, 3, 39, 70, 44, 39, 78, 45, 24, 44, 25, 9, 44, 44, 44, 96, 61, 44, 78, 44, 28, 36, 0, 44, 19, 39, 66, 81, 48, 39, 8, 91, 66, 72, 72, 50, 29, 39, 83, 73, 60, 84, 76, 39, 6, 7, 16, 39, 20, 86, 36, 27, 47, 7, 44, 36, 61, 71, 64, 39, 21, 59, 24, 33, 44, 40, 80, 39, 31, 44, 75, 89, 84, 23, 52, 0, 39, 21, 23, 57, 96, 39, 44, 39, 44, 23, 39, 48, 75, 42, 71, 44, 44, 5, 66, 28, 69, 79, 64, 61, 21, 33, 50, 41, 39, 45, 96, 64, 54, 44, 41, 94, 71, 67, 40, 61, 39, 9, 39, 69, 70, 39, 55, 42, 17, 17, 16, 74, 25, 91, 39, 51, 3, 56, 40, 39, 13, 72, 39, 92, 61, 8, 67, 24, 4, 79, 44, 39, 69, 7, 67, 61, 64, 39, 39, 89, 16, 25, 7, 92, 39, 36, 61, 89, 39, 39, 44, 55, 43, 72, 79, 11, 14, 39, 85, 55, 49, 70, 69, 77, 61, 12, 26, 15, 44, 61, 61, 21, 31, 58, 69, 92, 53, 61, 61, 64, 61, 80, 44, 44, 37, 79, 7, 20, 14, 21, 44, 43, 44, 23, 39, 59, 33, 48, 85, 9, 44, 39, 76, 79, 65, 37, 49, 44, 62, 17, 59, 79, 61, 61, 34, 32, 79, 68, 44, 75, 30, 45, 40, 44, 21, 2, 44, 30, 61, 94, 61, 96, 7, 44, 44, 39, 46, 31, 37, 17, 84, 25, 56, 67, 39, 10, 70, 89, 11, 25, 63, 10, 44, 73, 53, 39, 61, 35, 83, 78, 68, 1, 14, 60, 61, 72, 39, 4, 13, 43, 40, 44, 21, 65, 17, 56, 8, 59, 55, 40, 39, 70, 39, 44, 58, 73, 20, 44, 72, 77, 61, 64, 61, 44, 82, 65, 61, 19, 85, 59, 72, 44, 50, 64, 3, 26, 40, 31, 72, 79, 44, 6, 61, 62, 39, 2, 39, 95, 44, 65, 82, 21, 61, 88, 45, 61, 52, 0, 77, 61, 44, 39, 12, 37, 59, 61, 50, 70, 12, 78, 89, 77, 21, 94, 29, 84, 61, 19, 6, 49, 21, 61, 59, 44, 61, 44, 39, 17, 59, 72, 29, 24, 43, 17, 64, 25, 38, 40, 44, 39, 61, 52, 65, 80, 84, 24, 64, 12, 0, 13, 61, 39, 61, 51, 72, 44, 47, 93, 72, 44, 41, 61, 93, 50, 39, 7, 50, 29, 23, 33, 33, 44, 40, 49, 34, 26, 72, 54, 28, 52, 39, 74, 22, 28, 39, 58, 46, 44, 68, 44, 39, 50, 44, 44, 72, 72, 25, 39, 3, 61, 61, 31, 40, 88, 48, 38, 12, 61, 61, 56, 73, 83, 40, 39, 15, 40, 72, 87, 79, 44, 61, 71, 95, 40, 17, 32, 61, 11, 25, 39, 39, 39, 92, 41, 40, 39, 82, 22, 44, 56, 48, 79, 67, 61, 49, 44, 19, 44, 39, 49, 41, 25, 95, 61, 26, 72, 57, 15, 59, 39, 44, 44, 57, 39, 39, 73, 79, 1, 91, 74, 61, 77, 14, 44, 77, 36, 39, 60, 40, 44, 40, 55, 84, 13, 75, 18, 91, 83, 44, 73, 34, 61, 26, 32, 23, 39, 8, 88, 28, 58, 61, 89, 6, 29, 39, 51, 78, 39, 39, 72, 34, 66, 73, 44, 94, 39, 71, 3, 63, 39, 15, 87, 36, 0, 6, 44, 20, 56, 39, 66, 37, 0, 87, 28, 28, 47, 61, 72, 61, 52, 72, 80, 0, 12, 44, 59, 62, 2, 26, 44, 29, 61, 44, 81, 39, 27, 56, 44, 69, 78, 5, 48, 75, 77, 39, 93, 26, 79, 61, 44, 39, 39, 61, 88, 10, 72, 21, 37, 12, 61, 19, 50, 67, 44, 6, 47, 44, 44, 84, 0, 91, 73, 28, 39, 90, 31, 47, 39, 88, 22, 39, 12, 58, 40, 42, 39, 72, 11, 39, 39, 68, 10, 39, 61, 78, 72, 61, 77, 61, 39, 44, 54, 63, 28, 28, 32, 67, 63, 1, 44, 70, 5, 39, 39, 76, 44, 7, 59, 28, 95, 44, 17, 61, 44, 0, 39, 44, 44, 9, 53, 19, 61, 82, 44, 66, 4, 39, 72, 55, 77, 72, 39, 44, 39, 61, 21, 15, 72, 42, 38, 81, 67, 39, 44, 79, 81, 74, 39, 19, 92, 44, 44, 75, 44, 34, 39, 44, 3, 43, 12, 44, 18, 61, 44, 44, 57, 21, 96, 4, 21, 9, 81, 3, 29, 39, 28, 29, 72, 37, 48, 39, 61, 70, 61, 49, 9, 14, 39, 44, 19, 66, 39, 67, 7, 44, 42, 26, 84, 61, 90, 87, 13, 60, 56, 80, 66, 40, 19, 6, 39, 72, 50, 44, 65, 34, 39, 83, 83]\n"
          ]
        }
      ],
      "source": [
        "x_train, y_train = np.array([t[\"x\"] for t in train]), [t[\"y\"] for t in train]\n",
        "x_val, y_val = np.array([t[\"x\"] for t in val]), [t[\"y\"] for t in val]\n",
        "x_test, y_test = np.array([t[\"x\"] for t in test]), [t[\"y\"] for t in test]\n",
        "print(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc6W07QVYoaM"
      },
      "source": [
        "Pré-processe os dados como antes, certificando-se de que sejam float32 e normalizados entre 0 e 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnXaiAgJYoaQ"
      },
      "outputs": [],
      "source": [
        "# normalize data\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_val = x_val.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "# convert labels to one-hot vectors\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ordUucUKYoaS"
      },
      "source": [
        "Vamos fazer um resumo do que temos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcKjxgtyYoaT"
      },
      "outputs": [],
      "source": [
        "# summary\n",
        "print(\"finished loading %d images from %d categories\"%(len(data), num_classes))\n",
        "print(\"train / validation / test split: %d, %d, %d\"%(len(x_train), len(x_val), len(x_test)))\n",
        "print(\"training data shape: \", x_train.shape)\n",
        "print(\"training labels shape: \", y_train.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-P9MNPcYoaY"
      },
      "source": [
        "Se tudo funcionou corretamente, você deve ter carregado um monte de imagens e dividido em três conjuntos: `train`, `val`, e `test`. A forma dos dados de treinamento deve ser (`n`, 224, 224, 3) onde `n` né o tamanho do seu conjunto de treinamento, e os rótulos devem ser (`n`, `c`) onde `c` cé o número de classes (97 no caso de `101_ObjectCategories`. \n",
        "\n",
        "Observe que dividimos todos os dados em três subconjuntos – um conjunto de treinamento -- `train`, um conjunto de validação -- `val`, e um conjunto de teste -- `test`. A razão para isso é avaliar adequadamente a precisão do nosso classificador. Durante o treinamento, o otimizador usa o conjunto de validação para avaliar seu desempenho interno, a fim de determinar o gradiente sem overfitting ao conjunto de treinamento. O conjunto `test` é sempre retirado do algoritmo de treinamento e é usado apenas no final para avaliar a precisão final do nosso modelo.\n",
        "Vejamos rapidamente algumas imagens de amostra do nosso conjunto de dados.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y84SmM2CYoaZ"
      },
      "outputs": [],
      "source": [
        "images = [os.path.join(dp, f) for dp, dn, filenames in os.walk('/content/101_ObjectCategories') for f in filenames if os.path.splitext(f)[1].lower() in ['.jpg','.png','.jpeg']]\n",
        "idx = [int(len(images) * random.random()) for i in range(8)]\n",
        "imgs = [keras.utils.load_img(images[i], target_size=(224, 224)) for i in idx]\n",
        "concat_image = np.concatenate([np.asarray(img) for img in imgs], axis=1)\n",
        "plt.figure(figsize=(16,4))\n",
        "plt.title('Exemplo de imagens', fontweight =\"bold\")                                    \n",
        "plt.imshow(concat_image)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2s5qypkYoad"
      },
      "source": [
        "### First training a neural net from scratch\n",
        "\n",
        "Antes de fazer o aprendizado de transferência, vamos primeiro construir uma rede neural do zero para fazer a classificação em nosso conjunto de dados. Isso nos dará uma linha de base para comparar com nossa rede aprendida por transferência mais tarde.\n",
        "\n",
        "A rede que construiremos contém 4 camadas alternadas convolucional e de pool máximo, seguidas por um [dropout](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf) após cada par `conv/pooling`. Após a última camada de `pooling`, anexaremos uma camada totalmente conectada com 128 neurônios, outra camada `dropout` e, finalmente, uma camada de classificação `softmax` para nossas classes.\n",
        "\n",
        "Nossa função de perda será, como de costume, perda categórica de entropia cruzada, e nosso algoritmo de aprendizado será  [AdaDelta](https://arxiv.org/abs/1212.5701). Várias coisas sobre esta rede podem ser alteradas para obter melhor desempenho, talvez usar uma rede maior ou um otimizador diferente ajude, mas para os propósitos deste notebook, o objetivo é apenas obter uma compreensão de uma linha de base aproximada para fins de comparação e então não é necessário gastar muito tempo tentando otimizar esta rede.\n",
        "\n",
        "Ao compilar a rede, vamos executar `model.summary()` para obter um instantâneo de suas camadas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y41GiiYTYoaf"
      },
      "outputs": [],
      "source": [
        "# build the network\n",
        "model = Sequential()\n",
        "print(\"Input dimensions: \",x_train.shape[1:])\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej9IWCzxYoai"
      },
      "source": [
        "Criamos uma rede de tamanho médio com aproximadamente 1,2 milhão de pesos e vieses (os parâmetros). A maioria deles está levando para uma camada totalmente conectada pré-softmax \"dense_5\".\n",
        "Agora podemos ir em frente e treinar nosso modelo para 6 épocas com um tamanho de lote de 32. Também registraremos seu histórico para que possamos plotar a perda ao longo do tempo mais tarde.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIqHecNAYoaj"
      },
      "outputs": [],
      "source": [
        "# compile the model to use categorical cross-entropy loss function and adadelta optimizer\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size = 128,\n",
        "                    epochs = 30,\n",
        "                    validation_data=(x_val, y_val))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yG0CKOI1Yoao"
      },
      "source": [
        "Vamos plotar a perda de validação e a precisão da validação ao longo do tempo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CCPq_ndYoap"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(16,4))\n",
        "ax = fig.add_subplot(121)\n",
        "ax.plot(history.history[\"val_loss\"])\n",
        "ax.set_title(\"validation loss\")\n",
        "ax.set_xlabel(\"epochs\")\n",
        "\n",
        "ax2 = fig.add_subplot(122)\n",
        "ax2.plot(history.history[\"val_accuracy\"])\n",
        "ax2.set_title(\"validation accuracy\")\n",
        "ax2.set_xlabel(\"epochs\")\n",
        "ax2.set_ylim(0, 1)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GI7Mj6-RYoau"
      },
      "source": [
        "Observe que a perda de validação começa a aumentar após cerca de 16 épocas, embora a precisão da validação permaneça aproximadamente entre 40% e 50%. Isso sugere que nosso modelo começa a se ajustar por volta dessa época, e o melhor desempenho teria sido alcançado se tivéssemos parado cedo nessa época. No entanto, nossa precisão provavelmente não teria sido superior a 50%, e provavelmente inferior.\n",
        "Também podemos obter uma avaliação final executando nosso modelo no conjunto de treinamento. Fazendo isso, obtemos os seguintes resultados:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Itd5LDAYoav"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', loss)\n",
        "print('Test accuracy:', accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIwMY_ZXYoax"
      },
      "source": [
        "Por fim, vemos que atingimos uma precisão (top-1) de cerca de 49%. Isso não é tão ruim para 6000 imagens, considerando que se usássemos uma estratégia ingênua de fazer suposições aleatórias, teríamos obtido apenas cerca de 1% de precisão.\n",
        "\n",
        "## Transfer learning by starting with existing network\n",
        "\n",
        "Now we can move on to the main strategy for training an image classifier on our small dataset: by starting with a larger and already trained network.\n",
        "\n",
        "Agora podemos passar para a estratégia principal para treinar um classificador de imagens em nosso pequeno conjunto de dados: começando com uma rede maior e já treinada.\n",
        "Para começar, vamos carregar o VGG16 do keras, que foi treinado no ImageNet e os pesos salvos online. Se esta for sua primeira vez carregando o VGG16, você precisará esperar um pouco para que os pesos sejam baixados da web. Uma vez que a rede é carregada, podemos inspecionar novamente as camadas com o método\n",
        "`summary()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpUDAbxiYoay"
      },
      "outputs": [],
      "source": [
        "vgg = keras.applications.VGG16(weights='imagenet', include_top=True)\n",
        "vgg.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLXTofcNYoa2"
      },
      "source": [
        "Observe que o VGG16 é muito maior que a rede que construímos anteriormente. Ele contém 13 camadas convolucionais e duas camadas totalmente conectadas no final, e tem mais de 138 milhões de parâmetros, cerca de 100 vezes mais parâmetros do que a rede que fizemos acima. Como nossa primeira rede, a maioria dos parâmetros é armazenada nas conexões que levam à primeira camada totalmente conectada.\n",
        "\n",
        "O VGG16 foi feito para resolver o ImageNet e atinge uma [taxa de erro top 5 de 8,8%](https://github.com/jcjohnson/cnn-benchmarks), o que significa que 91,2% das amostras de teste foram classificadas corretamente dentro das 5 principais previsões para cada imagem. A precisão top 1 - equivalente à métrica de precisão que estamos usando (que a previsão principal está correta) - é de 73%. Isso é especialmente impressionante, pois não há apenas 97, mas 1.000 classes, o que significa que suposições aleatórias nos dariam apenas 0,1% de precisão.\n",
        "\n",
        "Para usar essa rede para nossa tarefa, \"removemos\" a camada de classificação final, a camada softmax de 1000 neurônios no final, que corresponde ao ImageNet, e a substituímos por uma nova camada softmax para nosso conjunto de dados, que contém 97 neurônios no caso do conjunto de dados 101_ObjectCategories.\n",
        "\n",
        "Em termos de implementação, é mais fácil simplesmente criar uma cópia do VGG de sua camada de entrada até a penúltima camada e trabalhar com isso, em vez de modificar o objeto VGG diretamente. Então, tecnicamente, nunca \"removemos\" nada, apenas contornamos/ignoramos. Isso pode ser feito da seguinte maneira, usando o `Model` classe keras para inicializar um novo modelo cuja camada de entrada é a mesma que VGG, mas cuja camada de saída é nossa nova camada softmax, chamada  `new_classification_layer`.\n",
        "\n",
        "Nota: embora pareça que estamos duplicando essa grande rede, internamente o Keras está apenas copiando todas as camadas por referência e, portanto, não precisamos nos preocupar em sobrecarregar a memória."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFL-fLitYoa3"
      },
      "outputs": [],
      "source": [
        "# make a reference to VGG's input layer\n",
        "inp = vgg.input\n",
        "\n",
        "# make a new softmax layer with num_classes neurons\n",
        "new_classification_layer = Dense(num_classes, activation='softmax')\n",
        "\n",
        "# connect our new layer to the second to last layer in VGG, and make a reference to it\n",
        "out = new_classification_layer(vgg.layers[-2].output)\n",
        "\n",
        "# create a new network between inp and out\n",
        "model_new = Model(inp, out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBIp3fbQYoa9"
      },
      "source": [
        "Vamos treinar novamente esta rede, `model_new` newno novo conjunto de dados e rótulos. Mas primeiro, precisamos congelar os pesos e desvios em todas as camadas da rede, exceto nossa nova no final, com a expectativa de que os recursos aprendidos no VGG ainda sejam bastante relevantes para a nova tarefa de classificação de imagens. Não é o ideal, mas provavelmente melhor do que podemos treinar em nosso conjunto de dados limitado. \n",
        "\n",
        "Ao definir o `trainable` trainablesinalizador em cada camada como false (exceto nossa nova camada de classificação), garantimos que todos os pesos e vieses nessas camadas permaneçam fixos e simplesmente treinamos os pesos em uma camada no final. Em alguns casos, é desejável não congelar todas as camadas de pré-classificação. Se o seu conjunto de dados tiver amostras suficientes e não se assemelhar muito ao ImageNet, pode ser vantajoso ajustar algumas das camadas VGG junto com o novo classificador, ou possivelmente até mesmo todas elas. Para fazer isso, você pode alterar o código abaixo para tornar mais das camadas treináveis.\n",
        "\n",
        "No caso do CalTech-101, faremos apenas a extração de recursos, temendo que o ajuste fino demais com esse conjunto de dados possa ser excessivo. Mas talvez estejamos errados? Um bom exercício seria experimentar ambos e comparar os resultados.\n",
        "\n",
        "Então vamos em frente e congelamos as camadas, e compilamos o novo modelo com exatamente o mesmo otimizador e função de perda que em nossa primeira rede, para uma comparação justa. Em seguida, corremos \n",
        " `summary` novamente para observar a arquitetura da rede."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_n5A8oGYoa9"
      },
      "outputs": [],
      "source": [
        "# make all layers untrainable by freezing weights (except for last layer)\n",
        "for l, layer in enumerate(model_new.layers[:-1]):\n",
        "    layer.trainable = False\n",
        "\n",
        "# ensure the last layer is trainable/not frozen\n",
        "for l, layer in enumerate(model_new.layers[-1:]):\n",
        "    layer.trainable = True\n",
        "\n",
        "model_new.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model_new.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8B9G0gC1YobD"
      },
      "source": [
        "Observando o resumo, vemos que a rede é idêntica ao modelo VGG que instanciamos anteriormente, exceto que a última camada, anteriormente um softmax de 1.000 neurônios, foi substituída por um novo softmax de 97 neurônios. Além disso, ainda temos cerca de 134 milhões de pesos, mas agora a grande maioria deles são \"parâmetros não treináveis\" porque congelamos as camadas em que estão contidos. Agora temos apenas 397.000 parâmetros treináveis, o que na verdade é apenas um quarto dos número de parâmetros necessários para treinar o primeiro modelo.\n",
        "\n",
        "Como antes, vamos em frente e treinamos o novo modelo, usando os mesmos hiperparâmetros (tamanho do lote e número de épocas) de antes, juntamente com o mesmo algoritmo de otimização. Também acompanhamos sua história à medida que avançamos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDdq71XNYobD"
      },
      "outputs": [],
      "source": [
        "history2 = model_new.fit(x_train, y_train, \n",
        "                         batch_size = 128, \n",
        "                         epochs = 30, \n",
        "                         validation_data=(x_val, y_val))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPqJ0OM8YobI"
      },
      "source": [
        "Nossa precisão de validação paira perto de 80% no final, o que é mais de 30% de melhoria na rede original treinada do zero (o que significa que fazemos a previsão errada em 20% das amostras, em vez de 50%).\n",
        "\n",
        "Vale a pena notar também que esta rede realmente treina um pouco mais rápido que a rede original, apesar de ter mais de 100 vezes mais parâmetros! Isso ocorre porque o congelamento dos pesos nega a necessidade de retropropagação por todas essas camadas, economizando tempo de execução.\n",
        "\n",
        "Vamos plotar a perda de validação e precisão novamente, desta vez comparando o modelo original treinado do zero (em azul) e o novo modelo aprendido por transferência em laranja.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHLdHnuuYobJ"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(16,4))\n",
        "ax = fig.add_subplot(121)\n",
        "ax.plot(history.history[\"val_loss\"])\n",
        "ax.plot(history2.history[\"val_loss\"])\n",
        "ax.set_title(\"validation loss\")\n",
        "ax.set_xlabel(\"epochs\")\n",
        "\n",
        "ax2 = fig.add_subplot(122)\n",
        "ax2.plot(history.history[\"val_accuracy\"])\n",
        "ax2.plot(history2.history[\"val_accuracy\"])\n",
        "ax2.set_title(\"validation accuracy\")\n",
        "ax2.set_xlabel(\"epochs\")\n",
        "ax2.set_ylim(0, 1)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXjfVTVIYobM"
      },
      "source": [
        "Observe que, enquanto o modelo original começou a se ajustar em torno da época 16, o novo modelo continuou a diminuir lentamente sua perda ao longo do tempo e provavelmente melhoraria ligeiramente sua precisão com mais iterações. O novo modelo chegou a aproximadamente 80% de precisão top-1 (no conjunto de validação) e continuou a melhorar lentamente por 30 épocas.\n",
        "\n",
        "É possível que pudéssemos ter melhorado o modelo original com melhor regularização ou mais abandono, mas certamente não teríamos compensado a melhoria >30% na precisão.\n",
        "Novamente, fazemos uma validação final no conjunto de teste.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMxC6Pd1YobN"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model_new.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print('Test loss:', loss)\n",
        "print('Test accuracy:', accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iUykardYobR"
      },
      "source": [
        "Para prever uma nova imagem, basta executar o código a seguir para obter as probabilidades de cada classe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpRcsywEYobT"
      },
      "outputs": [],
      "source": [
        "img, x = get_image('/content/101_ObjectCategories/Motorbikes/image_0003.jpg')\n",
        "probabilities = model_new.predict([x])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2ahKv8XYobY"
      },
      "source": [
        "### Melhorando os resultados\n",
        "\n",
        "78,2% de precisão top 1 em 97 classes, distribuídas de maneira uniforme, é uma conquista muito boa. Não é tão impressionante quanto o VGG16 original, que alcançou 73% de precisão no top 1 em 1000 classes. No entanto, é muito melhor do que conseguimos com nossa rede original e há espaço para melhorias. Algumas técnicas que possivelmente poderiam ter melhorado nosso desempenho.\n",
        "\n",
        "- Usando aumento de dados: aumento refere-se ao uso de várias modificações dos dados de treinamento originais, na forma de distorções, rotações, reescalonamento, mudanças de iluminação, etc., para aumentar o tamanho do conjunto de treinamento e criar mais tolerância para tais distorções.\n",
        "- Usando um otimizador diferente, adicionando mais regularização/desistência e outros hiperparâmetros.\n",
        "- Treinando por mais tempo (é claro)\n",
        "\n",
        "Um exemplo mais avançado de aprendizado de transferência no Keras, envolvendo aumento para um pequeno conjunto de dados de 2 classes, pode ser encontrado no [Blog do Keras](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "5d0bc67e0492eda9a079d01eee1a2d259cf3b68b49a414fff72c914d6eec3790"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}